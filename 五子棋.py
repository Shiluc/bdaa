import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
import os
import sys

# --- 1. å…¨å±€é…ç½® (ä¸“ä¸º 20x20 ä¼˜åŒ–) ---
BOARD_SIZE = 20  # âš¡ï¸ æ£‹ç›˜æ‰©å¤§åˆ° 20x20
LR = 0.001
MEM_CAPACITY = 10000  # è®°å¿†åº“ç¿»å€ï¼Œå› ä¸ºæ£‹ç›˜å˜å¤§æƒ…å†µå˜å¤š
BATCH_SIZE = 512  # âš¡ï¸ æ‰¹æ¬¡å¢å¤§ï¼Œå……åˆ†åˆ©ç”¨ 4060 æ˜¾å­˜
EPSILON = 0.9  # è®­ç»ƒæ—¶çš„è´ªå©ªåº¦
GAMMA = 0.95  # çœ‹å¾—æ›´è¿œ
TARGET_REPLACE_ITER = 500
MODEL_FILE = 'gomoku_20x20.pth'  # å­˜æ¡£æ–‡ä»¶å


# é¢œè‰²ä»£ç 
class Colors:
    RESET = "\033[0m"
    RED = "\033[91m"  # AI
    BLUE = "\033[94m"  # Human
    BOLD = "\033[1m"
    GRAY = "\033[90m"


# æ£€æµ‹ GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ è®¡ç®—è®¾å¤‡: {Colors.BOLD}{device}{Colors.RESET}")
if torch.cuda.is_available():
    print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")


# --- 2. 20x20 ç¯å¢ƒé€»è¾‘ ---
class GomokuEnv:
    def __init__(self):
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE))
        self.current_player = 1

    def reset(self):
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE))
        self.current_player = 1
        return self.board

    def step(self, action):
        x, y = action // BOARD_SIZE, action % BOARD_SIZE
        if self.board[x][y] != 0:
            return self.board, -10, True, {}  # æ— æ•ˆè½å­æƒ©ç½š

        self.board[x][y] = self.current_player

        if self.check_win(x, y, self.current_player):
            return self.board, 20, True, {'result': 'win'}  # èµ¢æ£‹å¥–åŠ±ç¿»å€

        if np.all(self.board != 0):
            return self.board, 0, True, {'result': 'draw'}

        return self.board, 0, False, {}

    def check_win(self, x, y, color):
        directions = [(0, 1), (1, 0), (1, 1), (1, -1)]
        for dx, dy in directions:
            count = 1
            for i in range(1, 5):
                nx, ny = x + dx * i, y + dy * i
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx][ny] == color:
                    count += 1
                else:
                    break
            for i in range(1, 5):
                nx, ny = x - dx * i, y - dy * i
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx][ny] == color:
                    count += 1
                else:
                    break
            if count >= 5: return True
        return False

    def get_valid_actions(self):
        return np.where(self.board.flatten() == 0)[0]


# --- 3. å¢å¼ºç‰ˆç¥ç»ç½‘ç»œ (é€‚é… 20x20) ---
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # é’ˆå¯¹ 20x20ï¼Œæˆ‘ä»¬éœ€è¦æ›´æ·±çš„ç½‘ç»œæ¥æ•æ‰ç‰¹å¾
        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, padding=2)  # æ„Ÿå—é‡å˜å¤§
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)  # å¢åŠ ç¬¬å››å±‚

        # å…¨è¿æ¥å±‚è¾“å…¥ç»´åº¦è®¡ç®—: 512ä¸ªé€šé“ * 20 * 20
        self.fc = nn.Linear(512 * BOARD_SIZE * BOARD_SIZE, BOARD_SIZE * BOARD_SIZE)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.relu(self.conv2(x))
        x = torch.relu(self.conv3(x))
        x = torch.relu(self.conv4(x))
        x = x.view(x.size(0), -1)
        actions_value = self.fc(x)
        return actions_value


# --- 4. DQN æ™ºèƒ½ä½“ ---
class DQN:
    def __init__(self):
        self.eval_net, self.target_net = Net().to(device), Net().to(device)
        self.learn_step = 0
        self.memory_counter = 0
        self.memory = np.zeros((MEM_CAPACITY, BOARD_SIZE * BOARD_SIZE * 2 + 2))
        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=LR)
        self.loss_func = nn.MSELoss()

    def choose_action(self, board, valid_actions, epsilon=EPSILON):
        board_tensor = torch.FloatTensor(board).view(1, 1, BOARD_SIZE, BOARD_SIZE).to(device)
        # è®­ç»ƒæ—¶ä½¿ç”¨ä¼ å…¥çš„ epsilonï¼Œå¯¹æˆ˜æ—¶é€šå¸¸ä¼ å…¥ 1.0 (ä¸éšæœº)
        if np.random.uniform() < epsilon:
            with torch.no_grad():
                actions_value = self.eval_net(board_tensor)
            action_probs = actions_value.cpu().numpy()[0]
            mask = np.full(BOARD_SIZE * BOARD_SIZE, -np.inf)
            mask[valid_actions] = action_probs[valid_actions]
            action = np.argmax(mask)
        else:
            action = np.random.choice(valid_actions)
        return action

    def store_transition(self, s, a, r, s_):
        transition = np.hstack((s.flatten(), [a, r], s_.flatten()))
        index = self.memory_counter % MEM_CAPACITY
        self.memory[index, :] = transition
        self.memory_counter += 1

    def learn(self):
        if self.learn_step % TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step += 1

        if self.memory_counter > MEM_CAPACITY:
            sample_index = np.random.choice(MEM_CAPACITY, BATCH_SIZE)
        else:
            sample_index = np.random.choice(self.memory_counter, BATCH_SIZE)

        b_memory = self.memory[sample_index, :]
        b_s = torch.FloatTensor(b_memory[:, :BOARD_SIZE * BOARD_SIZE]).view(-1, 1, BOARD_SIZE, BOARD_SIZE).to(device)
        b_a = torch.LongTensor(b_memory[:, BOARD_SIZE * BOARD_SIZE:BOARD_SIZE * BOARD_SIZE + 1].astype(int)).to(device)
        b_r = torch.FloatTensor(b_memory[:, BOARD_SIZE * BOARD_SIZE + 1:BOARD_SIZE * BOARD_SIZE + 2]).to(device)
        b_s_ = torch.FloatTensor(b_memory[:, -BOARD_SIZE * BOARD_SIZE:]).view(-1, 1, BOARD_SIZE, BOARD_SIZE).to(device)

        q_eval = self.eval_net(b_s).gather(1, b_a)
        q_next = self.target_net(b_s_).detach()
        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)

        loss = self.loss_func(q_eval, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    # --- å…³é”®ï¼šä¿å­˜ä¸åŠ è½½ ---
    def save_model(self):
        torch.save(self.eval_net.state_dict(), MODEL_FILE)

    def load_model(self):
        if os.path.exists(MODEL_FILE):
            self.eval_net.load_state_dict(torch.load(MODEL_FILE, map_location=device))
            self.target_net.load_state_dict(self.eval_net.state_dict())
            return True
        return False


# --- 5. è®­ç»ƒé€»è¾‘ ---
def train(bot):
    env = GomokuEnv()
    print(f"\nğŸš€ å¼€å§‹ 20x20 å¤§æ£‹ç›˜è®­ç»ƒ...")
    print(f"æç¤º: æ£‹ç›˜å˜å¤§4å€ï¼Œéš¾åº¦æŒ‡æ•°çº§ä¸Šå‡ã€‚")
    # è®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„è®­ç»ƒå±€æ•°ï¼Œå› ä¸ºä½ è¯´ä¸ç”¨æ‹…å¿ƒæ—¶é—´
    # 20x20 å¾ˆéš¾éšæœºæ’å‡ºèƒœåˆ©ï¼Œæ‰€ä»¥éœ€è¦æµ·é‡å¯¹å±€
    episodes = 20000
    print(f"ç›®æ ‡: {episodes} å±€ (è®­ç»ƒè¿‡ç¨‹ä¸­å¯éšæ—¶æŒ‰ Ctrl+C åœæ­¢ï¼Œä¼šè‡ªåŠ¨ä¿å­˜)")

    start_time = time.time()
    try:
        for episode in range(episodes):
            board = env.reset()
            done = False
            while not done:
                valid_actions = env.get_valid_actions()
                # è®­ç»ƒæ—¶ä½¿ç”¨ EPSILON (0.9) è¿›è¡Œéƒ¨åˆ†æ¢ç´¢
                action = bot.choose_action(board, valid_actions, epsilon=EPSILON)

                board_next, reward, done, info = env.step(action)

                if not done:
                    # å¯¹æ‰‹ç­–ç•¥ï¼šéšæœº
                    valid_actions = env.get_valid_actions()
                    opp_action = np.random.choice(valid_actions)
                    board_next, opp_reward, done, info = env.step(opp_action)
                    if done and info.get('result') == 'win':
                        reward = -10  # è¾“äº†æƒ©ç½š

                bot.store_transition(board, action, reward, board_next)

                if bot.memory_counter > BATCH_SIZE:
                    bot.learn()

                board = board_next

            if episode % 5 == 0:
                print(f"Episode: {episode}/{episodes} | è€—æ—¶: {time.time() - start_time:.0f}s")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ç´§æ€¥ä¿å­˜æ¨¡å‹...")

    bot.save_model()
    print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_FILE}")


# --- 6. æ˜¾ç¤ºä¸äº¤äº’ ---
def print_pretty_board(board):
    print("\n   ", end="")
    for i in range(BOARD_SIZE):
        print(f"{i % 10:2d}", end="")  # åªæ‰“å°ä¸ªä½æ•°é˜²æ­¢é”™ä½
    print("\n")
    for i in range(BOARD_SIZE):
        print(f"{i:2d} ", end="")
        for j in range(BOARD_SIZE):
            if board[i][j] == 1:
                print(f"{Colors.RED}ğŸ”´{Colors.RESET}", end="")  # ç´§å‡‘æ˜¾ç¤º
            elif board[i][j] == -1:
                print(f"{Colors.BLUE}ğŸ”µ{Colors.RESET}", end="")
            else:
                print(f"{Colors.GRAY} +{Colors.RESET}", end="")
        print("")


def human_vs_ai(bot):
    env = GomokuEnv()
    board = env.reset()
    print("\n" + "=" * 40)
    print(f"ğŸ® 20x20 å·…å³°å¯¹å†³")
    print(f"ä½ æ˜¯: {Colors.BLUE}ğŸ”µ{Colors.RESET}   AIæ˜¯: {Colors.RED}ğŸ”´{Colors.RESET}")
    print("=" * 40)

    ai_turn = True
    done = False

    while not done:
        print_pretty_board(board)

        if ai_turn:
            print(f"\n{Colors.RED}AI æ€è€ƒä¸­...{Colors.RESET}")
            valid_actions = env.get_valid_actions()
            # å¯¹æˆ˜æ—¶ epsilon=2.0 (å®Œå…¨è´ªå©ªï¼Œä¸éšæœº)
            action = bot.choose_action(board, valid_actions, epsilon=2.0)
            board, r, done, info = env.step(action)

            if done:
                print_pretty_board(board)
                print(f"\n{Colors.RED}AI èµ¢äº†ï¼{Colors.RESET}")
        else:
            while True:
                try:
                    move = input(f"\n{Colors.BLUE}ä½ çš„å›åˆ (è¡Œ åˆ—): {Colors.RESET}")
                    r, c = map(int, move.split())
                    if 0 <= r < BOARD_SIZE and 0 <= c < BOARD_SIZE:
                        if board[r][c] == 0:
                            action = r * BOARD_SIZE + c
                            board, r, done, info = env.step(action)
                            break
                        else:
                            print("è¿™é‡Œæœ‰å­äº†")
                    else:
                        print("åæ ‡è¶Šç•Œ")
                except:
                    print("è¾“å…¥é”™è¯¯")

            if done:
                print_pretty_board(board)
                print(f"\n{Colors.BLUE}ä½ èµ¢äº†ï¼{Colors.RESET}")

        ai_turn = not ai_turn


if __name__ == "__main__":
    bot = DQN()

    # --- æ ¸å¿ƒé€»è¾‘ï¼šæœ‰å­˜æ¡£å°±è¯»ï¼Œæ²¡å­˜æ¡£å°±è®­ ---
    if os.path.exists(MODEL_FILE):
        print(f"\nğŸ“‚ æ£€æµ‹åˆ°å­˜æ¡£ '{MODEL_FILE}'")
        print("âœ… åŠ è½½æˆåŠŸï¼è·³è¿‡è®­ç»ƒï¼Œç›´æ¥å¼€å§‹å¯¹æˆ˜ã€‚")
        bot.load_model()
    else:
        print(f"\nğŸš« æœªæ£€æµ‹åˆ°å­˜æ¡£ï¼Œåˆå§‹åŒ–è®­ç»ƒæ¨¡å¼...")
        train(bot)

    # æ— è®ºæ˜¯å¦åˆšåˆšè®­ç»ƒè¿‡ï¼Œéƒ½è¿›å…¥å¯¹æˆ˜
    human_vs_ai(bot)