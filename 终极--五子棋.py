import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
import os
import sys

# --- 1. ç»ˆæé…ç½® (ä¸è®¡æˆæœ¬æ¨¡å¼) ---
BOARD_SIZE = 20
LR = 0.0001  # é™ä½å­¦ä¹ ç‡ï¼Œç²¾ç»†æ‰“ç£¨
MEM_CAPACITY = 30000  # å·¨å¤§çš„è®°å¿†åº“
BATCH_SIZE = 1024  # âš¡ï¸ æ¦¨å¹²æ˜¾å­˜ï¼Œä¸€æ¬¡å­¦ 1024 æ­¥
EPSILON = 0.9
GAMMA = 0.99  # æå…¶çœ‹é‡é•¿è¿œåˆ©ç›Š
TARGET_REPLACE_ITER = 1000
MODEL_FILE = 'gomoku_god_mode.pth'


# é¢œè‰²ä»£ç 
class Colors:
    RESET = "\033[0m"
    RED = "\033[91m"  # AI
    BLUE = "\033[94m"  # Human
    BOLD = "\033[1m"
    GRAY = "\033[90m"


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ è®¡ç®—è®¾å¤‡: {Colors.BOLD}{device}{Colors.RESET}")
if torch.cuda.is_available():
    print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")


# --- 2. ä¿®å¤åçš„ç¯å¢ƒ ---
class GomokuEnv:
    def __init__(self):
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE))
        self.current_player = 1

    def reset(self):
        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE))
        self.current_player = 1
        return self.board

    def step(self, action):
        x, y = action // BOARD_SIZE, action % BOARD_SIZE
        if self.board[x][y] != 0:
            return self.board, -10, True, {}

        self.board[x][y] = self.current_player

        if self.check_win(x, y, self.current_player):
            return self.board, 100, True, {'result': 'win'}  # èµ¢æ£‹ç»™å·¨å¤§å¥–åŠ±

        if np.all(self.board != 0):
            return self.board, 0, True, {'result': 'draw'}

        # âœ… BUG ä¿®å¤ï¼šäº¤æ¢æ£‹æ‰‹
        self.current_player *= -1

        return self.board, 0, False, {}

    def check_win(self, x, y, color):
        directions = [(0, 1), (1, 0), (1, 1), (1, -1)]
        for dx, dy in directions:
            count = 1
            for i in range(1, 5):
                nx, ny = x + dx * i, y + dy * i
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx][ny] == color:
                    count += 1
                else:
                    break
            for i in range(1, 5):
                nx, ny = x - dx * i, y - dy * i
                if 0 <= nx < BOARD_SIZE and 0 <= ny < BOARD_SIZE and self.board[nx][ny] == color:
                    count += 1
                else:
                    break
            if count >= 5: return True
        return False

    def get_valid_actions(self):
        return np.where(self.board.flatten() == 0)[0]


# --- 3. ç»ˆæç½‘ç»œï¼šResNet + Dueling (å†³æ–—ç½‘ç»œ) ---
# æ®‹å·®å—ï¼šè®©ç½‘ç»œå¯ä»¥éå¸¸æ·±è€Œä¸é€€åŒ–
class ResBlock(nn.Module):
    def __init__(self, channels):
        super(ResBlock, self).__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        identity = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += identity  # å…³é”®ï¼šæ®‹å·®è¿æ¥
        out = self.relu(out)
        return out


class GodNet(nn.Module):
    def __init__(self):
        super(GodNet, self).__init__()
        # è¾“å…¥3é€šé“: [æˆ‘æ–¹æ£‹å­, æ•Œæ–¹æ£‹å­, å½“å‰æ˜¯å¦å¯è½å­]
        self.conv_input = nn.Sequential(
            nn.Conv2d(3, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )

        # å †å  10 å±‚ ResBlock (æ·±åº¦æ€è€ƒ)
        # å¦‚æœä½ ä¸è®¡æˆæœ¬ï¼Œå¯ä»¥åŠ åˆ° 20 å±‚
        self.res_blocks = nn.Sequential(*[ResBlock(128) for _ in range(10)])

        # Dueling DQN åˆ†æ”¯ 1: Value (è¯„ä¼°å½“å‰å±€åŠ¿å¥½å)
        self.value_head = nn.Sequential(
            nn.Conv2d(128, 1, kernel_size=1),
            nn.BatchNorm2d(1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(BOARD_SIZE * BOARD_SIZE, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

        # Dueling DQN åˆ†æ”¯ 2: Advantage (è¯„ä¼°æ¯ä¸ªåŠ¨ä½œçš„ä¼˜åŠ¿)
        self.adv_head = nn.Sequential(
            nn.Conv2d(128, 2, kernel_size=1),
            nn.BatchNorm2d(2),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(2 * BOARD_SIZE * BOARD_SIZE, 512),
            nn.ReLU(),
            nn.Linear(512, BOARD_SIZE * BOARD_SIZE)
        )

    def forward(self, x):
        x = self.conv_input(x)
        x = self.res_blocks(x)

        value = self.value_head(x)
        adv = self.adv_head(x)

        # Dueling åˆå¹¶å…¬å¼: Q = V + (A - mean(A))
        return value + adv - adv.mean(dim=1, keepdim=True)


# --- 4. Double Dueling DQN æ™ºèƒ½ä½“ ---
class Agent:
    def __init__(self):
        self.eval_net, self.target_net = GodNet().to(device), GodNet().to(device)
        self.target_net.load_state_dict(self.eval_net.state_dict())  # åŒæ­¥å‚æ•°
        self.learn_step = 0
        self.memory_counter = 0
        # å­˜å‚¨æ ¼å¼å˜äº†ï¼Œä¸éœ€è¦å­˜ board é‚£ä¹ˆå¤§çš„ flatï¼Œå­˜ç´¢å¼•å³å¯ï¼Œä½†ä¸ºäº†ç®€å•è¿™é‡Œè¿˜æ˜¯å­˜ raw
        # è¿™é‡Œä¸ºäº†æ˜¾å­˜ä¼˜åŒ–ï¼Œæˆ‘ä»¬åœ¨ learn çš„æ—¶å€™å†å¤„ç† tensor
        self.memory_s = np.zeros((MEM_CAPACITY, 3, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)
        self.memory_a = np.zeros(MEM_CAPACITY, dtype=np.int64)
        self.memory_r = np.zeros(MEM_CAPACITY, dtype=np.float32)
        self.memory_s_ = np.zeros((MEM_CAPACITY, 3, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)
        self.memory_done = np.zeros(MEM_CAPACITY, dtype=np.float32)

        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=LR)
        self.loss_func = nn.MSELoss()

    def board_to_state(self, board, player):
        # å°†æ£‹ç›˜è½¬æ¢ä¸º 3 é€šé“ Tensor
        # Channel 0: è‡ªå·±çš„å­ (1)
        # Channel 1: å¯¹æ‰‹çš„å­ (-1)
        # Channel 2: ç©ºåœ° (0) æˆ– å…¨1 bias
        state = np.zeros((3, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)
        state[0] = (board == player).astype(float)
        state[1] = (board == -player).astype(float)
        state[2] = (board == 0).astype(float)  # å¯è¡ŒåŒºåŸŸ
        return state

    def choose_action(self, board, valid_actions, player, epsilon=EPSILON):
        state = self.board_to_state(board, player)
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

        if np.random.uniform() < epsilon:
            with torch.no_grad():
                actions_value = self.eval_net(state_tensor)
            action_probs = actions_value.cpu().numpy()[0]
            mask = np.full(BOARD_SIZE * BOARD_SIZE, -np.inf)
            mask[valid_actions] = action_probs[valid_actions]
            action = np.argmax(mask)
        else:
            action = np.random.choice(valid_actions)
        return action

    def store_transition(self, board, a, r, board_next, done, player):
        index = self.memory_counter % MEM_CAPACITY
        # å­˜å‚¨æ—¶è½¬æ¢çŠ¶æ€ï¼ŒèŠ‚çœåç»­è®¡ç®—
        self.memory_s[index] = self.board_to_state(board, player)
        self.memory_a[index] = a
        self.memory_r[index] = r
        # ä¸‹ä¸€çŠ¶æ€å¯¹äºå½“å‰ç©å®¶æ¥è¯´ï¼Œè§†è§’æ˜¯ä¸å˜çš„ï¼ˆè¿˜æ˜¯ Channel 0 æ˜¯è‡ªå·±ï¼‰
        # ä½†æ˜¯ï¼ä¸‹ä¸€æ­¥è½®åˆ°å¯¹æ‰‹ä¸‹ï¼Œæ‰€ä»¥å¯¹äºé¢„æµ‹æ¥è¯´ï¼Œè¦é¢„æµ‹å¯¹æ‰‹çš„è¡ŒåŠ¨å—ï¼Ÿ
        # è¿™é‡Œä½¿ç”¨æ ‡å‡† DQN é€»è¾‘ï¼šState Next æ˜¯å®¢è§‚ç›˜é¢
        self.memory_s_[index] = self.board_to_state(board_next, player)
        self.memory_done[index] = 1.0 if done else 0.0
        self.memory_counter += 1

    def learn(self):
        if self.learn_step % TARGET_REPLACE_ITER == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step += 1

        if self.memory_counter > MEM_CAPACITY:
            sample_index = np.random.choice(MEM_CAPACITY, BATCH_SIZE)
        else:
            sample_index = np.random.choice(self.memory_counter, BATCH_SIZE)

        b_s = torch.tensor(self.memory_s[sample_index], device=device)
        b_a = torch.tensor(self.memory_a[sample_index], device=device).unsqueeze(1)
        b_r = torch.tensor(self.memory_r[sample_index], device=device).unsqueeze(1)
        b_s_ = torch.tensor(self.memory_s_[sample_index], device=device)
        b_done = torch.tensor(self.memory_done[sample_index], device=device).unsqueeze(1)

        # --- Double DQN æ ¸å¿ƒé€»è¾‘ ---
        # 1. ç”¨ Eval Net é€‰å‡º s_ çŠ¶æ€ä¸‹æœ€å¥½çš„åŠ¨ä½œ argmax(Q_eval)
        q_next_eval = self.eval_net(b_s_)
        max_act4next = q_next_eval.argmax(dim=1, keepdim=True)

        # 2. ç”¨ Target Net è®¡ç®—è¿™ä¸ªåŠ¨ä½œçš„ä»·å€¼ Q_target
        q_next_target = self.target_net(b_s_).gather(1, max_act4next)

        # 3. è®¡ç®—ç›®æ ‡å€¼
        q_target = b_r + GAMMA * q_next_target * (1 - b_done)

        # 4. å½“å‰é¢„æµ‹å€¼
        q_eval = self.eval_net(b_s).gather(1, b_a)

        loss = self.loss_func(q_eval, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def save_model(self):
        torch.save(self.eval_net.state_dict(), MODEL_FILE)

    def load_model(self):
        if os.path.exists(MODEL_FILE):
            self.eval_net.load_state_dict(torch.load(MODEL_FILE, map_location=device))
            self.target_net.load_state_dict(self.eval_net.state_dict())
            return True
        return False


# --- 5. è®­ç»ƒå‡½æ•° ---
def train(bot):
    env = GomokuEnv()
    print(f"\nğŸš€ å¯åŠ¨ç¥çº§è®­ç»ƒæ¨¡å¼ (ResNet + Double Dueling DQN)...")
    episodes = 50000

    start_time = time.time()
    try:
        for episode in range(episodes):
            board = env.reset()
            done = False

            while not done:
                player = env.current_player  # è®°å½•å½“å‰æ˜¯è°
                valid_actions = env.get_valid_actions()

                # è®­ç»ƒæ—¶ AI è‡ªå·±è·Ÿè‡ªå·±ä¸‹ (Self-Play)
                # ä½¿ç”¨åŒä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œä½†åˆ†åˆ«æ‰®æ¼”é»‘ç™½æ£‹
                action = bot.choose_action(board, valid_actions, player, epsilon=EPSILON)

                board_next, reward, done, info = env.step(action)

                # å­˜å‚¨ç»éªŒ
                bot.store_transition(board, action, reward, board_next, done, player)

                # åªè¦æ•°æ®å¤Ÿå°±ç–¯ç‹‚å­¦ä¹ 
                if bot.memory_counter > BATCH_SIZE:
                    bot.learn()

                board = board_next

            if episode % 10 == 0:
                elapsed = time.time() - start_time
                speed = (episode + 1) / (elapsed + 1e-5)
                print(
                    f"\rEp: {episode}/{episodes} | Time: {elapsed:.0f}s | Speed: {speed:.2f} G/s | Mem: {bot.memory_counter}",
                    end="")

    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒæš‚åœï¼Œä¿å­˜ä¸­...")

    print("\n")
    bot.save_model()
    print(f"ğŸ’¾ ç¥çº§æ¨¡å‹å·²ä¿å­˜è‡³ {MODEL_FILE}")
    return bot


# --- 6. æ˜¾ç¤ºä¸å¯¹æˆ˜ ---
def print_pretty_board(board):
    print("\n   ", end="")
    for i in range(BOARD_SIZE): print(f"{i % 10:2d}", end="")
    print("\n")
    for i in range(BOARD_SIZE):
        print(f"{i:2d} ", end="")
        for j in range(BOARD_SIZE):
            if board[i][j] == 1:
                print(f"{Colors.RED}ğŸ”´{Colors.RESET}", end="")
            elif board[i][j] == -1:
                print(f"{Colors.BLUE}ğŸ”µ{Colors.RESET}", end="")
            else:
                print(f"{Colors.GRAY} +{Colors.RESET}", end="")
        print("")


def human_vs_ai(bot):
    env = GomokuEnv()
    board = env.reset()
    print(f"\nğŸ® æŒ‘æˆ˜ç¥çº§ AI (20x20)")
    print(f"ä½ : {Colors.BLUE}ğŸ”µ{Colors.RESET} vs AI: {Colors.RED}ğŸ”´{Colors.RESET}")

    # éšæœºå…ˆæ‰‹
    ai_turn = random.choice([True, False])
    if ai_turn:
        print("ğŸ‘‰ AI å…ˆæ‰‹")
    else:
        print("ğŸ‘‰ ä½ å…ˆæ‰‹")

    done = False
    while not done:
        print_pretty_board(board)

        if ai_turn:
            print(f"\n{Colors.RED}AI æ­£åœ¨è®¡ç®—...{Colors.RESET}")
            valid_actions = env.get_valid_actions()
            # è¿™é‡Œçš„ 1 ä»£è¡¨ AI æ‰§é»‘ (å¦‚æœ AI å…ˆæ‰‹)ï¼Œæˆ–è€… AI æ‰§ç™½ (å¦‚æœ AI åæ‰‹)
            # åœ¨æˆ‘ä»¬çš„ Env é‡Œï¼Œå½“å‰è¡ŒåŠ¨è€…æ€»æ˜¯ self.current_player
            # æˆ‘ä»¬çš„ choose_action éœ€è¦çŸ¥é“ board å’Œ player
            action = bot.choose_action(board, valid_actions, env.current_player, epsilon=1.0)
            board, r, done, info = env.step(action)

            if done:
                print_pretty_board(board)
                print(f"\n{Colors.RED}AI èµ¢äº†ï¼{Colors.RESET}")
        else:
            while True:
                try:
                    move = input(f"\n{Colors.BLUE}è½å­ (è¡Œ åˆ—): {Colors.RESET}")
                    r, c = map(int, move.split())
                    if board[r][c] == 0:
                        action = r * BOARD_SIZE + c
                        board, r, done, info = env.step(action)
                        break
                    else:
                        print("âŒ æ— æ•ˆä½ç½®")
                except:
                    print("âŒ æ ¼å¼é”™è¯¯")

            if done:
                print_pretty_board(board)
                print(f"\n{Colors.BLUE}ä½ èµ¢äº†ï¼{Colors.RESET}")

        ai_turn = not ai_turn


if __name__ == "__main__":
    bot = Agent()
    if os.path.exists(MODEL_FILE):
        print(f"\nğŸ“‚ åŠ è½½ç¥çº§æ¨¡å‹...")
        bot.load_model()
    else:
        train(bot)
    human_vs_ai(bot)