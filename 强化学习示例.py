import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time
import random

# --- 1. æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨ ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ”¥ å½“å‰ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {device}")
if device.type == 'cuda':
    print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")

# --- 2. æ¸¸æˆå‚æ•° ---
N_STATES = 6  # åœ°å›¾é•¿åº¦
N_ACTIONS = 2  # åŠ¨ä½œ: 0(å·¦), 1(å³)
EPSILON = 0.9  # è´ªå©ªåº¦
GAMMA = 0.9  # å¥–åŠ±è¡°å‡
LR = 0.01  # å­¦ä¹ ç‡
MEMORY_CAPACITY = 200  # è®°å¿†åº“å¤§å°
BATCH_SIZE = 32  # æ¯æ¬¡ä»è®°å¿†åº“æŠ½å¤šå°‘æ¡æ•°æ®ç»™GPUè®­ç»ƒ


# --- 3. å®šä¹‰ç¥ç»ç½‘ç»œ (å¤§è„‘) ---
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œ
        # è¾“å…¥æ˜¯çŠ¶æ€(ä½ç½®çš„One-hotç¼–ç )ï¼Œè¾“å‡ºæ˜¯æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼
        self.fc1 = nn.Linear(N_STATES, 50)  # ç¬¬ä¸€å±‚ï¼š50ä¸ªç¥ç»å…ƒ
        self.fc1.weight.data.normal_(0, 0.1)
        self.out = nn.Linear(50, N_ACTIONS)  # è¾“å‡ºå±‚ï¼š2ä¸ªåŠ¨ä½œ
        self.out.weight.data.normal_(0, 0.1)

    def forward(self, x):
        x = self.fc1(x)
        x = torch.relu(x)  # æ¿€æ´»å‡½æ•°
        actions_value = self.out(x)
        return actions_value


# --- 4. å®šä¹‰ DQN æ™ºèƒ½ä½“ ---
class DQN(object):
    def __init__(self):
        # ä¸¤ä¸ªç½‘ç»œï¼ševal_netç”¨äºå†³ç­–ï¼Œtarget_netç”¨äºè®¡ç®—ç›®æ ‡
        self.eval_net, self.target_net = Net().to(device), Net().to(device)
        self.learn_step_counter = 0
        self.memory_counter = 0
        # åˆå§‹åŒ–è®°å¿†åº“ (å…¨0)
        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))
        self.optimizer = optim.Adam(self.eval_net.parameters(), lr=LR)
        self.loss_func = nn.MSELoss()

    def choose_action(self, x):
        # å°†ç®€å•çš„æ•°å­—ä½ç½® (æ¯”å¦‚ 2) è½¬æ¢æˆ One-hot å‘é‡ (0,0,1,0,0,0)
        # è¿™æ ·ç¥ç»ç½‘ç»œæ‰èƒ½ç†è§£
        x = torch.unsqueeze(torch.FloatTensor(x).to(device), 0)

        if np.random.uniform() < EPSILON:  # è´ªå©ªç­–ç•¥
            actions_value = self.eval_net.forward(x)
            # é€‰ä»·å€¼æœ€å¤§çš„åŠ¨ä½œ
            action = torch.max(actions_value, 1)[1].data.cpu().numpy()[0]
        else:  # éšæœºç­–ç•¥
            action = np.random.randint(0, N_ACTIONS)
        return action

    def store_transition(self, s, a, r, s_):
        # å­˜å‚¨è®°å¿†ï¼š[å½“å‰çŠ¶æ€, åŠ¨ä½œ, å¥–åŠ±, ä¸‹ä¸€çŠ¶æ€]
        transition = np.hstack((s, [a, r], s_))
        # å¦‚æœè®°å¿†åº“æ»¡äº†ï¼Œå°±è¦†ç›–æ—§çš„
        index = self.memory_counter % MEMORY_CAPACITY
        self.memory[index, :] = transition
        self.memory_counter += 1

    def learn(self):
        # ç›®æ ‡ç½‘ç»œæ¯100æ­¥æ›´æ–°ä¸€æ¬¡
        if self.learn_step_counter % 100 == 0:
            self.target_net.load_state_dict(self.eval_net.state_dict())
        self.learn_step_counter += 1

        # éšæœºæŠ½å–ä¸€æ‰¹è®°å¿†æ•°æ®
        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)
        b_memory = self.memory[sample_index, :]

        # å°†æ•°æ®æ¬è¿åˆ° GPU
        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)
        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES + 1].astype(int)).to(device)
        b_r = torch.FloatTensor(b_memory[:, N_STATES + 1:N_STATES + 2]).to(device)
        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)

        # q_eval: ç¥ç»ç½‘ç»œè®¡ç®—å‡ºçš„ å½“å‰çŠ¶æ€-å½“å‰åŠ¨ä½œ çš„ä»·å€¼
        q_eval = self.eval_net(b_s).gather(1, b_a)
        # q_next: ç¥ç»ç½‘ç»œè®¡ç®—å‡ºçš„ ä¸‹ä¸€æ­¥çŠ¶æ€ çš„æœ€å¤§ä»·å€¼ï¼ˆä¸åå‘ä¼ æ’­ï¼‰
        q_next = self.target_net(b_s_).detach()
        # q_target: ç°å®ä¸–ç•Œçš„å¥–åŠ± + æœªæ¥çš„é¢„æœŸ
        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)

        # è®¡ç®—è¯¯å·®å¹¶åå‘ä¼ æ’­
        loss = self.loss_func(q_eval, q_target)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


# --- è¾…åŠ©å‡½æ•°ï¼šå°†ä½ç½®è½¬æ¢ä¸º One-hot å‘é‡ ---
def state_to_onehot(state):
    one_hot = np.zeros(N_STATES)
    if state != 'terminal':
        one_hot[state] = 1.0
    return one_hot


# --- ç¯å¢ƒåé¦ˆ (å’Œä¹‹å‰ä¸€æ ·) ---
def get_env_feedback(S, A):
    if A == 1:  # å‘å³
        if S == N_STATES - 2:
            S_ = 'terminal'
            R = 1
        else:
            S_ = S + 1
            R = 0
    else:  # å‘å·¦
        R = 0
        if S == 0:
            S_ = S
        else:
            S_ = S - 1
    return S_, R


def update_env(S, episode, step_counter):
    env_list = ['-'] * (N_STATES - 1) + ['T']
    if S == 'terminal':
        interaction = 'Episode %s: total_steps = %s' % (episode + 1, step_counter)
        print('\r{}'.format(interaction), end='', flush=True)
        time.sleep(0.5)
        print('\r                                ', end='', flush=True)
    else:
        env_list[S] = 'o'
        interaction = ''.join(env_list)
        print('\r{}'.format(interaction), end='', flush=True)
        time.sleep(0.05)  # ç¨å¾®å¿«ä¸€ç‚¹


# --- ä¸»å¾ªç¯ ---
dqn = DQN()

print("ğŸ¤– ç¥ç»ç½‘ç»œ(DQN) æ­£åœ¨åˆå§‹åŒ–...")
print("ğŸš€ æ­£åœ¨æ”¶é›†åˆå§‹ç»éªŒ(å…ˆä¹±èµ°ä¸€ä¼š)...")

for episode in range(200):  # å¢åŠ å›åˆæ•°ï¼Œå› ä¸ºç¥ç»ç½‘ç»œéœ€è¦æ›´å¤šæ•°æ®
    S = 0  # åˆå§‹ä½ç½®
    step_counter = 0
    # å°†æ•°å­—ä½ç½®è½¬ä¸ºå‘é‡
    S_vec = state_to_onehot(S)

    while True:
        # æ˜¾ç¤ºåŠ¨ç”» (å‰10è½®æˆ–è€…æ¯20è½®æ˜¾ç¤ºä¸€æ¬¡ï¼Œä¸ç„¶å¤ªæ…¢)
        if episode < 10 or episode % 20 == 0:
            update_env(S, episode, step_counter)

        # 1. ç¥ç»ç½‘ç»œé€‰åŠ¨ä½œ
        A = dqn.choose_action(S_vec)

        # 2. ç¯å¢ƒåé¦ˆ
        S_, R = get_env_feedback(S, A)

        # å¤„ç†ä¸‹ä¸€çŠ¶æ€çš„å‘é‡
        S_vec_next = state_to_onehot(S_ if S_ != 'terminal' else 0)  # ç»ˆç‚¹éšä¾¿ç»™ä¸ªå ä½ç¬¦
        if S_ == 'terminal':
            # ç»ˆç‚¹ä¹Ÿæ˜¯å…¨0å‘é‡ï¼Œæˆ–è€…ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œç®€å•å¤„ç†ä¸ºå…¨0
            S_vec_next = np.zeros(N_STATES)

        # 3. å­˜å…¥è®°å¿†åº“
        dqn.store_transition(S_vec, A, R, S_vec_next)

        # 4. è®°å¿†åº“å¤Ÿäº†å°±å¼€å§‹å­¦ä¹ 
        if dqn.memory_counter > MEMORY_CAPACITY:
            dqn.learn()

        if S_ == 'terminal':
            break

        S = S_
        S_vec = S_vec_next
        step_counter += 1

print("\nğŸ‰ è®­ç»ƒç»“æŸï¼")